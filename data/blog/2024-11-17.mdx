---
title: 'How did I provision and deploy basic three-tier web application to AWS using Terraform'
date: '2024-11-17'
lastmod: '2024-11-17'
tags: ['AWS', 'Terraform', 'DevOps', 'ECS', 'Aurora']
draft: false
summary: 'Deploy Application Load Balancer, ECS Fargate, and Aurora RDS using Terraform'
---

# Terraform: Provision and Deploy Basic Three-Tier Web Application to AWS

In this blog post, I'm going to share how I provisioned and deployed a basic three-tier web application to AWS using Terraform. The application consists of an Application Load Balancer (ALB) to distribute incoming traffic, an ECS Fargate cluster to run the application containers, and an Aurora RDS database to store application data. I'll walk you through the steps I took to set up the infrastructure and deploy the application using Terraform.

## Prerequisites

I'm assuming you have the following prerequisites:

- An AWS CLI profile configured with the necessary permissions.
- Terraform installed on your local machine.

## Step 1: Create GitHub Repository and Initialize Terraform

I created a new GitHub private repository to store the Terraform configuration files. Initially I included .gitignore file like below:

````gitignore
# Local .terraform directories
**/.terraform/*

# .tfstate files
*.tfstate
*.tfstate.*

# Crash log files
crash.log
crash.*.log

# Exclude all .tfvars files, which are likely to contain sensitive data, such as
# password, private keys, and other secrets. These should not be part of version
# control as they are data points which are potentially sensitive and subject
# to change depending on the environment.
*.tfvars
*.tfvars.json

# Ignore override files as they are usually used to override resources locally and so
# are not checked in
override.tf
override.tf.json
*_override.tf
*_override.tf.json

# Ignore transient lock info files created by terraform apply
.terraform.tfstate.lock.info

# Include override files you do wish to add to version control using negated pattern
# !example_override.tf

# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan
# example: *tfplan*

# Ignore CLI configuration files
.terraformrc
terraform.rc


```hcl

````

.tfvars is just like environment variables, but it's used for Terraform. It's a good practice to keep sensitive information like access keys, secret keys, and other secrets in .tfvars files and exclude them from version control.

Then I created S3 bucket and DynamoDB table to store Terraform state file and lock file. I used AWS CLI with following command (bucket name is different) to create the bucket and table:

```bash
aws s3api create-bucket --bucket my-terraform-bucket --region ap-northeast-2 --create-bucket-configuration LocationConstraint=ap-northeast-2

# Enable versioning on the bucket (optional but recommended)

aws s3api put-bucket-versioning --bucket my-terraform-bucket --versioning-configuration Status=Enabled

# Create a DynamoDB table for state locking

aws dynamodb create-table --table-name terraform-state-lock --attribute-definitions AttributeName=LockID,AttributeType=S \
--key-schema AttributeName=LockID,KeyType=HASH --billing-mode PAY_PER_REQUEST
```

After creating the bucket and table, I added backend.tf file, in ./mock-web-app directory, which contains the configuration for remote state management using S3 bucket and DynamoDB table:

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    cloudflare = {
      source  = "cloudflare/cloudflare"
      version = "~> 3.0"
    }
  }

  backend "s3" {
    bucket         = "my-terraform-bucket"    # S3 bucket name
    key            = "mock-web-app/terraform.tfstate" # Folder/Key structure in S3
    region         = "ap-northeast-2"
    dynamodb_table = "terraform-state-lock" # State locking table
    encrypt        = true                   # Encrypt the state file
  }
}
```

And below is how the directory structure looks after the end of this blog post:

```
root/
├── mock-web-app/               # Main folder for the application
│   ├── alb.tf                  # ALB resources (HTTP/HTTPS listeners, Target Groups)
│   ├── backend.tf              # Remote state management
│   ├── database.tf             # Aurora database resources
│   ├── ecs.tf                  # ECS cluster, task definition, and service
│   ├── ecr.tf                  # ECR repository
│   ├── main.tf                 # Main Terraform configuration
│   ├── outputs.tf              # Outputs for ALB DNS, database endpoints, etc.
│   ├── provider.tf             # AWS provider configuration
│   ├── variables.tf            # Input variables for resources
│   └── terraform.tfvars        # Values for variables
├── .gitignore                  # Git ignore file
├── README.md                   # Project README
```

## Step 2: Import Existing VPC and Subnets

I have an existing VPC and subnets in my AWS account that I want to use for the application. So instead of creating vpc and other networking resources using Terraform, I would import the VPC and subnets into the Terraform state.

First I created a new file called main.tf in the mock-web-app directory and added the following code
to import the VPC and subnets:

```hcl
provider "aws" {
  region = "ap-northeast-2" # Ensure it matches your backend region
}

data "aws_vpc" "practice" {
  filter {
    name   = "tag:Name"       # Use the tag "Name" to identify the VPC
    values = ["practice-vpc"] # The name of your VPC
  }

}

resource "aws_subnet" "public_subnet_a" {
  vpc_id                  = data.aws_vpc.practice.id
  cidr_block              = "172.0.32.0/24"
  availability_zone       = "ap-northeast-2a"
  map_public_ip_on_launch = true

  tags = {
    Name  = "PublicSubnetA"
    Stage = "prod"
  }
  lifecycle {
    prevent_destroy = true
  }
}


resource "aws_subnet" "public_subnet_b" {
  vpc_id                  = data.aws_vpc.practice.id
  cidr_block              = "172.0.33.0/24"
  availability_zone       = "ap-northeast-2b"
  map_public_ip_on_launch = true

  tags = {
    Name  = "PublicSubnetB"
    Stage = "prod"
  }
  lifecycle {
    prevent_destroy = true
  }
}


resource "aws_subnet" "private_subnet_a" {
  vpc_id                  = data.aws_vpc.practice.id
  cidr_block              = "172.0.0.0/20"
  availability_zone       = "ap-northeast-2a"
  map_public_ip_on_launch = false

  tags = {
    Name  = "PrivateSubnetA"
    Stage = "prod"
  }
  lifecycle {
    prevent_destroy = true
  }
}

resource "aws_subnet" "private_subnet_b" {
  vpc_id                  = data.aws_vpc.practice.id
  cidr_block              = "172.0.16.0/20"
  availability_zone       = "ap-northeast-2b"
  map_public_ip_on_launch = false

  tags = {
    Name  = "PrivateSubnetB"
    Stage = "prod"
  }
  lifecycle {
    prevent_destroy = true
  }
}

```
